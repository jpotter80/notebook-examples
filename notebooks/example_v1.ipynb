{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Data Cleaning with DuckDB\n",
    "## Data Quality Assessment and Cleaning Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a systematic approach to cleaning e-commerce product data \n",
    "spread across multiple files and formats. We'll use DuckDB as our primary processing \n",
    "engine, taking advantage of its ability to directly query different file formats\n",
    "and perform efficient transformations.\n",
    "\n",
    "Our source files:\n",
    "1. main_catalog.csv - Primary product catalog\n",
    "2. inventory_update.xlsx - Recent inventory updates\n",
    "3. price_list.json - Latest pricing information\n",
    "4. category_mapping.parquet - Category standardization mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up our environment and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define project directory structure.\n",
    "Note: Adjust this base path according to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.home() / 'freelance'\n",
    "DATA_DIR = PROJECT_ROOT / 'testing' / 'synthetic-data' / 'ecommerce'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify data directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create path objects for our data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_CATALOG = DATA_DIR / 'main_catalog.csv'\n",
    "INVENTORY_UPDATE = DATA_DIR / 'inventory_update.xlsx'\n",
    "PRICE_LIST = DATA_DIR / 'price_list.json'\n",
    "CATEGORY_MAPPING = DATA_DIR / 'category_mapping.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify all required files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files found:\n",
      "- Main Catalog: /home/james/freelance/testing/synthetic-data/ecommerce/main_catalog.csv\n",
      "- Inventory Update: /home/james/freelance/testing/synthetic-data/ecommerce/inventory_update.xlsx\n",
      "- Price List: /home/james/freelance/testing/synthetic-data/ecommerce/price_list.json\n",
      "- Category Mapping: /home/james/freelance/testing/synthetic-data/ecommerce/category_mapping.parquet\n"
     ]
    }
   ],
   "source": [
    "required_files = {\n",
    "    'Main Catalog': MAIN_CATALOG,\n",
    "    'Inventory Update': INVENTORY_UPDATE,\n",
    "    'Price List': PRICE_LIST,\n",
    "    'Category Mapping': CATEGORY_MAPPING\n",
    "}\n",
    "\n",
    "missing_files = [name for name, path in required_files.items() if not path.exists()]\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n",
    "\n",
    "print(\"Data files found:\")\n",
    "for name, path in required_files.items():\n",
    "    print(f\"- {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize DuckDB and load required extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = duckdb.connect(':memory:')\n",
    "conn.execute(\"INSTALL spatial;\")\n",
    "conn.execute(\"LOAD spatial;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create standardized SKUs table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS clean_skus AS\n",
    "WITH all_skus AS (\n",
    "    SELECT DISTINCT sku FROM (\n",
    "        SELECT sku FROM '{MAIN_CATALOG}'\n",
    "        UNION ALL\n",
    "        SELECT sku FROM st_read('{INVENTORY_UPDATE}')\n",
    "        UNION ALL\n",
    "        SELECT sku FROM read_json('{PRICE_LIST}', format='auto')\n",
    "        UNION ALL\n",
    "        SELECT sku FROM '{CATEGORY_MAPPING}'\n",
    "    ) t\n",
    "    WHERE sku IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    sku as original_sku,\n",
    "    CASE \n",
    "        WHEN LENGTH(sku) = 10 \n",
    "        AND SUBSTRING(sku, 3, 1) = '-'\n",
    "        AND SUBSTRING(sku, 6, 1) = '-'\n",
    "        AND SUBSTRING(sku, 7) ~ '^[0-9]+$'\n",
    "        THEN sku\n",
    "        ELSE UPPER(SUBSTRING(sku, 1, 2)) || '-' ||\n",
    "             UPPER(SUBSTRING(sku, 4, 2)) || '-' ||\n",
    "             LPAD(REGEXP_REPLACE(SUBSTRING(sku, 7), '[^0-9]', '', 'g'), 4, '0')\n",
    "    END as clean_sku\n",
    "FROM all_skus;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify SKU cleaning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU Cleaning Results:\n",
      "   total_skus  unique_original_skus  unique_clean_skus\n",
      "0        9187                  9187               9187\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_skus,\n",
    "    COUNT(DISTINCT original_sku) as unique_original_skus,\n",
    "    COUNT(DISTINCT clean_sku) as unique_clean_skus\n",
    "FROM clean_skus;\n",
    "\"\"\")\n",
    "print(\"SKU Cleaning Results:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create standardized prices table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS clean_prices AS\n",
    "WITH price_sources AS (\n",
    "    SELECT \n",
    "        cs.clean_sku,\n",
    "        CASE \n",
    "            WHEN mc.price LIKE '$%' THEN CAST(REPLACE(mc.price, '$', '') AS DECIMAL(10,2))\n",
    "            WHEN mc.price IS NOT NULL THEN CAST(mc.price AS DECIMAL(10,2))\n",
    "            ELSE NULL\n",
    "        END as catalog_price,\n",
    "        CASE \n",
    "            WHEN pl.price LIKE '$%' THEN CAST(REPLACE(pl.price, '$', '') AS DECIMAL(10,2))\n",
    "            WHEN pl.price IS NOT NULL THEN CAST(pl.price AS DECIMAL(10,2))\n",
    "            ELSE NULL\n",
    "        END as list_price\n",
    "    FROM clean_skus cs\n",
    "    LEFT JOIN '{MAIN_CATALOG}' mc ON cs.original_sku = mc.sku\n",
    "    LEFT JOIN read_json('{PRICE_LIST}', format='auto') pl ON cs.original_sku = pl.sku\n",
    ")\n",
    "SELECT \n",
    "    clean_sku,\n",
    "    COALESCE(list_price, catalog_price) as final_price,\n",
    "    catalog_price IS NOT NULL as had_catalog_price,\n",
    "    list_price IS NOT NULL as had_list_price\n",
    "FROM price_sources;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify price cleaning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price Cleaning Results:\n",
      "   total_prices  unique_skus  avg_price  from_catalog  from_price_list\n",
      "0          9521         9187     253.87          5450             3771\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_prices,\n",
    "    COUNT(DISTINCT clean_sku) as unique_skus,\n",
    "    ROUND(AVG(final_price), 2) as avg_price,\n",
    "    COUNT(*) FILTER (WHERE had_catalog_price) as from_catalog,\n",
    "    COUNT(*) FILTER (WHERE had_list_price) as from_price_list\n",
    "FROM clean_prices;\n",
    "\"\"\")\n",
    "print(\"\\nPrice Cleaning Results:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create standardized inventory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS clean_inventory AS\n",
    "WITH inventory_sources AS (\n",
    "    SELECT \n",
    "        cs.clean_sku,\n",
    "        GREATEST(COALESCE(mc.inventory, 0), 0) as catalog_inventory,\n",
    "        mc.last_updated as catalog_date,\n",
    "        GREATEST(COALESCE(iu.inventory, 0), 0) as update_inventory,\n",
    "        iu.last_updated as update_date\n",
    "    FROM clean_skus cs\n",
    "    LEFT JOIN '{MAIN_CATALOG}' mc ON cs.original_sku = mc.sku\n",
    "    LEFT JOIN st_read('{INVENTORY_UPDATE}') iu ON cs.original_sku = iu.sku\n",
    ")\n",
    "SELECT \n",
    "    clean_sku,\n",
    "    CASE \n",
    "        WHEN update_date > catalog_date OR catalog_date IS NULL THEN update_inventory\n",
    "        ELSE catalog_inventory\n",
    "    END as final_inventory,\n",
    "    GREATEST(update_date, catalog_date) as last_updated\n",
    "FROM inventory_sources;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify inventory cleaning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inventory Cleaning Results:\n",
      "   total_inventory_records  unique_skus  avg_inventory  in_stock_items\n",
      "0                     9463         9187          29.63            5620\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_inventory_records,\n",
    "    COUNT(DISTINCT clean_sku) as unique_skus,\n",
    "    ROUND(AVG(final_inventory), 2) as avg_inventory,\n",
    "    COUNT(*) FILTER (WHERE final_inventory > 0) as in_stock_items\n",
    "FROM clean_inventory;\n",
    "\"\"\")\n",
    "print(\"\\nInventory Cleaning Results:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create standardized categories table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS clean_categories AS\n",
    "WITH category_sources AS (\n",
    "    SELECT \n",
    "        cs.clean_sku,\n",
    "        COALESCE(\n",
    "            -- Custom capitalization: uppercase first letter, lowercase rest\n",
    "            CONCAT(UPPER(LEFT(TRIM(cm.category), 1)), LOWER(RIGHT(TRIM(cm.category), LENGTH(TRIM(cm.category))-1))),\n",
    "            CONCAT(UPPER(LEFT(TRIM(mc.category), 1)), LOWER(RIGHT(TRIM(mc.category), LENGTH(TRIM(mc.category))-1)))\n",
    "        ) as category,\n",
    "        COALESCE(cm.subcategory, mc.subcategory) as subcategory\n",
    "    FROM clean_skus cs\n",
    "    LEFT JOIN '{CATEGORY_MAPPING}' cm ON cs.original_sku = cm.sku\n",
    "    LEFT JOIN '{MAIN_CATALOG}' mc ON cs.original_sku = mc.sku\n",
    ")\n",
    "SELECT \n",
    "    clean_sku,\n",
    "    category,\n",
    "    subcategory\n",
    "FROM category_sources;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify category cleaning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category Cleaning Results:\n",
      "   total_category_records  unique_skus  unique_categories  \\\n",
      "0                    9625         9187                  7   \n",
      "\n",
      "   unique_subcategories  \n",
      "0                    26  \n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_category_records,\n",
    "    COUNT(DISTINCT clean_sku) as unique_skus,\n",
    "    COUNT(DISTINCT category) as unique_categories,\n",
    "    COUNT(DISTINCT subcategory) as unique_subcategories\n",
    "FROM clean_categories;\n",
    "\"\"\")\n",
    "print(\"\\nCategory Cleaning Results:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at our category values to verify the capitalization worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of Cleaned Categories:\n",
      "  category    subcategory\n",
      "0             Accessories\n",
      "1              Appliances\n",
      "2           Arts & Crafts\n",
      "3                   Audio\n",
      "4                Building\n",
      "5                 Cameras\n",
      "6                Children\n",
      "7                Clothing\n",
      "8                Cookware\n",
      "9                   Decor\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT DISTINCT category, subcategory\n",
    "FROM clean_categories\n",
    "WHERE category IS NOT NULL\n",
    "ORDER BY category, subcategory\n",
    "LIMIT 10;\n",
    "\"\"\")\n",
    "print(\"\\nSample of Cleaned Categories:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create final combined table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS combined_products AS\n",
    "SELECT \n",
    "    s.clean_sku,\n",
    "    s.original_sku,\n",
    "    -- Get product name from main catalog\n",
    "    mc.product_name,\n",
    "    -- Get standardized category and subcategory\n",
    "    c.category,\n",
    "    c.subcategory,\n",
    "    -- Get cleaned price\n",
    "    p.final_price as price,\n",
    "    -- Get cleaned inventory\n",
    "    i.final_inventory as inventory,\n",
    "    i.last_updated as inventory_date,\n",
    "    -- Add data quality flags\n",
    "    CASE \n",
    "        WHEN p.final_price IS NULL THEN 'Missing price'\n",
    "        WHEN i.final_inventory IS NULL THEN 'Missing inventory'\n",
    "        WHEN c.category IS NULL THEN 'Missing category'\n",
    "        WHEN c.subcategory IS NULL THEN 'Missing subcategory'\n",
    "        ELSE 'Complete'\n",
    "    END as record_status\n",
    "FROM clean_skus s\n",
    "LEFT JOIN '{MAIN_CATALOG}' mc ON s.original_sku = mc.sku\n",
    "LEFT JOIN clean_prices p ON s.clean_sku = p.clean_sku\n",
    "LEFT JOIN clean_inventory i ON s.clean_sku = i.clean_sku\n",
    "LEFT JOIN clean_categories c ON s.clean_sku = c.clean_sku;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record Completeness Analysis:\n",
      "         record_status  count  percentage\n",
      "0             Complete  12402       77.36\n",
      "1        Missing price   2863       17.86\n",
      "2  Missing subcategory    767        4.78\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    record_status,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "FROM combined_products\n",
    "GROUP BY record_status\n",
    "ORDER BY count DESC;\n",
    "\"\"\")\n",
    "print(\"\\nRecord Completeness Analysis:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check value ranges in combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Dataset Statistics:\n",
      "   total_records  unique_products  min_price  max_price  avg_price  \\\n",
      "0          16032             9187      10.09     504.68      265.4   \n",
      "\n",
      "   min_inventory  max_inventory  avg_inventory  unique_categories  \\\n",
      "0            0.0           99.0          36.95                  7   \n",
      "\n",
      "   unique_subcategories  \n",
      "0                    26  \n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT clean_sku) as unique_products,\n",
    "    ROUND(MIN(price), 2) as min_price,\n",
    "    ROUND(MAX(price), 2) as max_price,\n",
    "    ROUND(AVG(price), 2) as avg_price,\n",
    "    MIN(inventory) as min_inventory,\n",
    "    MAX(inventory) as max_inventory,\n",
    "    ROUND(AVG(inventory), 2) as avg_inventory,\n",
    "    COUNT(DISTINCT category) as unique_categories,\n",
    "    COUNT(DISTINCT subcategory) as unique_subcategories\n",
    "FROM combined_products;\n",
    "\"\"\")\n",
    "print(\"\\nCombined Dataset Statistics:\")\n",
    "print(conn.fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the cleaned dataset.\n",
    "For Excel export, we'll cast decimals to float to avoid GDAL issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW export_view AS\n",
    "SELECT \n",
    "    clean_sku,\n",
    "    original_sku,\n",
    "    product_name,\n",
    "    category,\n",
    "    subcategory,\n",
    "    CAST(price AS FLOAT) as price,\n",
    "    CAST(inventory AS INTEGER) as inventory,\n",
    "    inventory_date,\n",
    "    record_status\n",
    "FROM combined_products;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "COPY combined_products \n",
    "TO '{DATA_DIR}/cleaned_combined_products.parquet' \n",
    "(FORMAT PARQUET);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c8b421a9370>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "COPY combined_products \n",
    "TO '{DATA_DIR}/cleaned_combined_products.csv'\n",
    "(HEADER, DELIMITER ',');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as Excel (using modified view for compatibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exported cleaned dataset to:\n",
      "- /home/james/freelance/testing/synthetic-data/ecommerce/cleaned_combined_products.parquet (full precision)\n",
      "- /home/james/freelance/testing/synthetic-data/ecommerce/cleaned_combined_products.csv (full precision)\n",
      "- /home/james/freelance/testing/synthetic-data/ecommerce/cleaned_combined_products.xlsx (floating point precision)\n"
     ]
    }
   ],
   "source": [
    "conn.execute(f\"\"\"\n",
    "COPY export_view \n",
    "TO '{DATA_DIR}/cleaned_combined_products.xlsx'\n",
    "(FORMAT GDAL, DRIVER 'xlsx');\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nExported cleaned dataset to:\")\n",
    "print(f\"- {DATA_DIR}/cleaned_combined_products.parquet (full precision)\")\n",
    "print(f\"- {DATA_DIR}/cleaned_combined_products.csv (full precision)\")\n",
    "print(f\"- {DATA_DIR}/cleaned_combined_products.xlsx (floating point precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the exports worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export Verification:\n",
      "   row_count  unique_skus  min_price   max_price  min_inventory  max_inventory\n",
      "0      16032         9187      10.09  504.679993              0             99\n"
     ]
    }
   ],
   "source": [
    "conn.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as row_count,\n",
    "    COUNT(DISTINCT clean_sku) as unique_skus,\n",
    "    MIN(price) as min_price,\n",
    "    MAX(price) as max_price,\n",
    "    MIN(inventory) as min_inventory,\n",
    "    MAX(inventory) as max_inventory\n",
    "FROM export_view;\n",
    "\"\"\")\n",
    "print(\"\\nExport Verification:\")\n",
    "print(conn.fetchdf())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
